{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import get_loss_function , calculate_metrics_all, load_data, plot_scatter, FlexibleMLP, Autoencoder, set_seed\n",
    "from train import CombinedModel, CustomDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"mol\": 'all', \"P_spectra_dim\": 101, \n",
    "     \"input_csv_path\": '/root/CPP_Customize_Transfer_Learning/data/demo/train_x.csv',\n",
    "     \"label_csv_path\": '/root/CPP_Customize_Transfer_Learning/data/demo/train_y.csv',\n",
    "     } \n",
    "\n",
    "file_paths = ['/root/CPP_Customize_Transfer_Learning/data/P_mol_spectra/mol1-p.xlsx', \n",
    "              '/root/CPP_Customize_Transfer_Learning/data/P_mol_spectra/mol2-p.xlsx', \n",
    "              '/root/CPP_Customize_Transfer_Learning/data/P_mol_spectra/mol3-p.xlsx']\n",
    "\n",
    "autoencoder_checkpoint = '/root/CPP_Customize_Transfer_Learning/ckpt/autoencoder/1500.pth'\n",
    "ckpt_dir = \"/root/CPP_Customize_Transfer_Learning/ckpt/demo\"\n",
    "results_file_path = '/root/CPP_Customize_Transfer_Learning/result/base_results_summary.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 125\n",
    "output_dim_autoencoder = 121\n",
    "loss_function_name = 'mae' \n",
    "sampling_frequency = 15\n",
    "freeze = False\n",
    "output_dim_combined = 501\n",
    "onehot_dim = 7  \n",
    "batch_size = 128\n",
    "learning_rate = 5e-5\n",
    "lr_decay_step = 200  \n",
    "lr_decay_gamma = 0.7 \n",
    "augment_times = 10\n",
    "noise_std = 1e-7\n",
    "test_size = 0.1\n",
    "k_folds = 10\n",
    "num_epochs = 300\n",
    "random_seed = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch [1/300], Loss: 0.8475263118743896, LR: 5e-05\n",
      " Epoch [2/300], Loss: 0.7922291874885559, LR: 5e-05\n",
      " Epoch [3/300], Loss: 0.7040727972984314, LR: 5e-05\n",
      " Epoch [4/300], Loss: 0.6087739884853363, LR: 5e-05\n",
      " Epoch [5/300], Loss: 0.5484233617782592, LR: 5e-05\n",
      " Epoch [6/300], Loss: 0.5280864804983139, LR: 5e-05\n",
      " Epoch [7/300], Loss: 0.5119353652000427, LR: 5e-05\n",
      " Epoch [8/300], Loss: 0.4949608951807022, LR: 5e-05\n",
      " Epoch [9/300], Loss: 0.468742698431015, LR: 5e-05\n",
      " Epoch [10/300], Loss: 0.41805826127529144, LR: 5e-05\n",
      " Epoch [11/300], Loss: 0.3713924527168274, LR: 5e-05\n",
      " Epoch [12/300], Loss: 0.3412347912788391, LR: 5e-05\n",
      " Epoch [13/300], Loss: 0.31182089149951936, LR: 5e-05\n",
      " Epoch [14/300], Loss: 0.29267154932022094, LR: 5e-05\n",
      " Epoch [15/300], Loss: 0.28103497326374055, LR: 5e-05\n",
      " Epoch [16/300], Loss: 0.27368472814559935, LR: 5e-05\n",
      " Epoch [17/300], Loss: 0.2715899258852005, LR: 5e-05\n",
      " Epoch [18/300], Loss: 0.2676327958703041, LR: 5e-05\n",
      " Epoch [19/300], Loss: 0.2653378590941429, LR: 5e-05\n",
      " Epoch [20/300], Loss: 0.26569627821445463, LR: 5e-05\n",
      " Epoch [21/300], Loss: 0.2624686032533646, LR: 5e-05\n",
      " Epoch [22/300], Loss: 0.26004156321287153, LR: 5e-05\n",
      " Epoch [23/300], Loss: 0.2584075003862381, LR: 5e-05\n",
      " Epoch [24/300], Loss: 0.2589841902256012, LR: 5e-05\n",
      " Epoch [25/300], Loss: 0.25755255222320556, LR: 5e-05\n",
      " Epoch [26/300], Loss: 0.2544863626360893, LR: 5e-05\n",
      " Epoch [27/300], Loss: 0.25218887627124786, LR: 5e-05\n",
      " Epoch [28/300], Loss: 0.2517950102686882, LR: 5e-05\n",
      " Epoch [29/300], Loss: 0.25023671239614487, LR: 5e-05\n",
      " Epoch [30/300], Loss: 0.24875722527503968, LR: 5e-05\n",
      " Epoch [31/300], Loss: 0.24797749966382981, LR: 5e-05\n",
      " Epoch [32/300], Loss: 0.2447440505027771, LR: 5e-05\n",
      " Epoch [33/300], Loss: 0.24361489415168763, LR: 5e-05\n",
      " Epoch [34/300], Loss: 0.24439613968133928, LR: 5e-05\n",
      " Epoch [35/300], Loss: 0.24113768935203553, LR: 5e-05\n",
      " Epoch [36/300], Loss: 0.23632568418979644, LR: 5e-05\n",
      " Epoch [37/300], Loss: 0.23173797875642776, LR: 5e-05\n",
      " Epoch [38/300], Loss: 0.23157986104488373, LR: 5e-05\n",
      " Epoch [39/300], Loss: 0.2281376302242279, LR: 5e-05\n",
      " Epoch [40/300], Loss: 0.22324669361114502, LR: 5e-05\n",
      " Epoch [41/300], Loss: 0.22051747143268585, LR: 5e-05\n",
      " Epoch [42/300], Loss: 0.21856837570667267, LR: 5e-05\n",
      " Epoch [43/300], Loss: 0.21604101359844208, LR: 5e-05\n",
      " Epoch [44/300], Loss: 0.21358393430709838, LR: 5e-05\n",
      " Epoch [45/300], Loss: 0.20965042263269423, LR: 5e-05\n",
      " Epoch [46/300], Loss: 0.20817460268735885, LR: 5e-05\n",
      " Epoch [47/300], Loss: 0.2040781319141388, LR: 5e-05\n",
      " Epoch [48/300], Loss: 0.19980238378047943, LR: 5e-05\n",
      " Epoch [49/300], Loss: 0.19903795123100282, LR: 5e-05\n",
      " Epoch [50/300], Loss: 0.1978884980082512, LR: 5e-05\n",
      " Epoch [51/300], Loss: 0.19438518285751344, LR: 5e-05\n",
      " Epoch [52/300], Loss: 0.19039784818887712, LR: 5e-05\n",
      " Epoch [53/300], Loss: 0.19096958935260772, LR: 5e-05\n",
      " Epoch [54/300], Loss: 0.19135115444660186, LR: 5e-05\n",
      " Epoch [55/300], Loss: 0.18620824664831162, LR: 5e-05\n",
      " Epoch [56/300], Loss: 0.1819911554455757, LR: 5e-05\n",
      " Epoch [57/300], Loss: 0.18032939434051515, LR: 5e-05\n",
      " Epoch [58/300], Loss: 0.17898809313774108, LR: 5e-05\n",
      " Epoch [59/300], Loss: 0.17823705226182937, LR: 5e-05\n",
      " Epoch [60/300], Loss: 0.17418847978115082, LR: 5e-05\n",
      " Epoch [61/300], Loss: 0.17248508632183074, LR: 5e-05\n",
      " Epoch [62/300], Loss: 0.172379931807518, LR: 5e-05\n",
      " Epoch [63/300], Loss: 0.16924722045660018, LR: 5e-05\n",
      " Epoch [64/300], Loss: 0.16935017108917236, LR: 5e-05\n",
      " Epoch [65/300], Loss: 0.1656200721859932, LR: 5e-05\n",
      " Epoch [66/300], Loss: 0.16482713371515273, LR: 5e-05\n",
      " Epoch [67/300], Loss: 0.16500223875045777, LR: 5e-05\n",
      " Epoch [68/300], Loss: 0.16071923226118087, LR: 5e-05\n",
      " Epoch [69/300], Loss: 0.161239854991436, LR: 5e-05\n",
      " Epoch [70/300], Loss: 0.15590195506811141, LR: 5e-05\n",
      " Epoch [71/300], Loss: 0.15434014052152634, LR: 5e-05\n",
      " Epoch [72/300], Loss: 0.15647571831941604, LR: 5e-05\n",
      " Epoch [73/300], Loss: 0.15493751615285872, LR: 5e-05\n",
      " Epoch [74/300], Loss: 0.15116286128759385, LR: 5e-05\n",
      " Epoch [75/300], Loss: 0.14767342656850815, LR: 5e-05\n",
      " Epoch [76/300], Loss: 0.14548838287591934, LR: 5e-05\n",
      " Epoch [77/300], Loss: 0.14166196882724763, LR: 5e-05\n",
      " Epoch [78/300], Loss: 0.14013862013816833, LR: 5e-05\n",
      " Epoch [79/300], Loss: 0.13851888179779054, LR: 5e-05\n",
      " Epoch [80/300], Loss: 0.13548674136400224, LR: 5e-05\n",
      " Epoch [81/300], Loss: 0.13541089743375778, LR: 5e-05\n",
      " Epoch [82/300], Loss: 0.13290946930646896, LR: 5e-05\n",
      " Epoch [83/300], Loss: 0.1308713138103485, LR: 5e-05\n",
      " Epoch [84/300], Loss: 0.12766587734222412, LR: 5e-05\n",
      " Epoch [85/300], Loss: 0.12587032169103624, LR: 5e-05\n",
      " Epoch [86/300], Loss: 0.12171153947710991, LR: 5e-05\n",
      " Epoch [87/300], Loss: 0.1209350883960724, LR: 5e-05\n",
      " Epoch [88/300], Loss: 0.11973386183381081, LR: 5e-05\n",
      " Epoch [89/300], Loss: 0.1187485434114933, LR: 5e-05\n",
      " Epoch [90/300], Loss: 0.11635455712676049, LR: 5e-05\n",
      " Epoch [91/300], Loss: 0.11483837440609931, LR: 5e-05\n",
      " Epoch [92/300], Loss: 0.114920025318861, LR: 5e-05\n",
      " Epoch [93/300], Loss: 0.1103926233947277, LR: 5e-05\n",
      " Epoch [94/300], Loss: 0.1075535349547863, LR: 5e-05\n",
      " Epoch [95/300], Loss: 0.1064530849456787, LR: 5e-05\n",
      " Epoch [96/300], Loss: 0.10488558784127236, LR: 5e-05\n",
      " Epoch [97/300], Loss: 0.1073801949620247, LR: 5e-05\n",
      " Epoch [98/300], Loss: 0.10657757446169853, LR: 5e-05\n",
      " Epoch [99/300], Loss: 0.10012959390878677, LR: 5e-05\n",
      " Epoch [100/300], Loss: 0.09734475612640381, LR: 5e-05\n",
      " Epoch [101/300], Loss: 0.0966963715851307, LR: 5e-05\n",
      " Epoch [102/300], Loss: 0.09830788522958755, LR: 5e-05\n",
      " Epoch [103/300], Loss: 0.09583989083766938, LR: 5e-05\n",
      " Epoch [104/300], Loss: 0.09263547882437706, LR: 5e-05\n",
      " Epoch [105/300], Loss: 0.09198941141366959, LR: 5e-05\n",
      " Epoch [106/300], Loss: 0.08977510184049606, LR: 5e-05\n",
      " Epoch [107/300], Loss: 0.0931233897805214, LR: 5e-05\n",
      " Epoch [108/300], Loss: 0.09226579293608665, LR: 5e-05\n",
      " Epoch [109/300], Loss: 0.08929933086037636, LR: 5e-05\n",
      " Epoch [110/300], Loss: 0.0893185406923294, LR: 5e-05\n",
      " Epoch [111/300], Loss: 0.09050811007618904, LR: 5e-05\n",
      " Epoch [112/300], Loss: 0.08832615613937378, LR: 5e-05\n",
      " Epoch [113/300], Loss: 0.08715512081980706, LR: 5e-05\n",
      " Epoch [114/300], Loss: 0.08508943691849709, LR: 5e-05\n",
      " Epoch [115/300], Loss: 0.08267380222678185, LR: 5e-05\n",
      " Epoch [116/300], Loss: 0.08379643186926841, LR: 5e-05\n",
      " Epoch [117/300], Loss: 0.0814564809203148, LR: 5e-05\n",
      " Epoch [118/300], Loss: 0.07883770763874054, LR: 5e-05\n",
      " Epoch [119/300], Loss: 0.0796394944190979, LR: 5e-05\n",
      " Epoch [120/300], Loss: 0.07937536761164665, LR: 5e-05\n",
      " Epoch [121/300], Loss: 0.07770055383443833, LR: 5e-05\n",
      " Epoch [122/300], Loss: 0.07574667781591415, LR: 5e-05\n",
      " Epoch [123/300], Loss: 0.07629140615463256, LR: 5e-05\n",
      " Epoch [124/300], Loss: 0.07670311778783798, LR: 5e-05\n",
      " Epoch [125/300], Loss: 0.07463462725281715, LR: 5e-05\n",
      " Epoch [126/300], Loss: 0.07702849954366683, LR: 5e-05\n",
      " Epoch [127/300], Loss: 0.07859685346484184, LR: 5e-05\n",
      " Epoch [128/300], Loss: 0.07406586259603501, LR: 5e-05\n",
      " Epoch [129/300], Loss: 0.0718408327549696, LR: 5e-05\n",
      " Epoch [130/300], Loss: 0.06849204078316688, LR: 5e-05\n",
      " Epoch [131/300], Loss: 0.06976289711892605, LR: 5e-05\n",
      " Epoch [132/300], Loss: 0.07043748013675213, LR: 5e-05\n",
      " Epoch [133/300], Loss: 0.07054835595190526, LR: 5e-05\n",
      " Epoch [134/300], Loss: 0.06926513463258743, LR: 5e-05\n",
      " Epoch [135/300], Loss: 0.06668896377086639, LR: 5e-05\n",
      " Epoch [136/300], Loss: 0.06618557907640935, LR: 5e-05\n",
      " Epoch [137/300], Loss: 0.06496204957365989, LR: 5e-05\n",
      " Epoch [138/300], Loss: 0.0665848445147276, LR: 5e-05\n",
      " Epoch [139/300], Loss: 0.06529899500310421, LR: 5e-05\n",
      " Epoch [140/300], Loss: 0.06475493237376213, LR: 5e-05\n",
      " Epoch [141/300], Loss: 0.06066781207919121, LR: 5e-05\n",
      " Epoch [142/300], Loss: 0.06228441074490547, LR: 5e-05\n",
      " Epoch [143/300], Loss: 0.06022023223340511, LR: 5e-05\n",
      " Epoch [144/300], Loss: 0.06081300675868988, LR: 5e-05\n",
      " Epoch [145/300], Loss: 0.061455100774765015, LR: 5e-05\n",
      " Epoch [146/300], Loss: 0.06060696691274643, LR: 5e-05\n",
      " Epoch [147/300], Loss: 0.060220641642808916, LR: 5e-05\n",
      " Epoch [148/300], Loss: 0.0590561393648386, LR: 5e-05\n",
      " Epoch [149/300], Loss: 0.05910987854003906, LR: 5e-05\n",
      " Epoch [150/300], Loss: 0.058412962779402736, LR: 5e-05\n",
      " Epoch [151/300], Loss: 0.05846266113221645, LR: 5e-05\n",
      " Epoch [152/300], Loss: 0.0596756711602211, LR: 5e-05\n",
      " Epoch [153/300], Loss: 0.05804834142327309, LR: 5e-05\n",
      " Epoch [154/300], Loss: 0.06002050526440143, LR: 5e-05\n",
      " Epoch [155/300], Loss: 0.05725716650485992, LR: 5e-05\n",
      " Epoch [156/300], Loss: 0.0549462765455246, LR: 5e-05\n",
      " Epoch [157/300], Loss: 0.05382942743599415, LR: 5e-05\n",
      " Epoch [158/300], Loss: 0.052890722453594205, LR: 5e-05\n",
      " Epoch [159/300], Loss: 0.054886290803551674, LR: 5e-05\n",
      " Epoch [160/300], Loss: 0.05580853261053562, LR: 5e-05\n",
      " Epoch [161/300], Loss: 0.05476497374475002, LR: 5e-05\n",
      " Epoch [162/300], Loss: 0.05618744865059853, LR: 5e-05\n",
      " Epoch [163/300], Loss: 0.05848565585911274, LR: 5e-05\n",
      " Epoch [164/300], Loss: 0.053151560574769975, LR: 5e-05\n",
      " Epoch [165/300], Loss: 0.05105492398142815, LR: 5e-05\n",
      " Epoch [166/300], Loss: 0.05041623041033745, LR: 5e-05\n",
      " Epoch [167/300], Loss: 0.0535038348287344, LR: 5e-05\n",
      " Epoch [168/300], Loss: 0.05264790393412113, LR: 5e-05\n",
      " Epoch [169/300], Loss: 0.051664486154913904, LR: 5e-05\n",
      " Epoch [170/300], Loss: 0.05316309109330177, LR: 5e-05\n",
      " Epoch [171/300], Loss: 0.05053553767502308, LR: 5e-05\n",
      " Epoch [172/300], Loss: 0.05000444874167442, LR: 5e-05\n",
      " Epoch [173/300], Loss: 0.05097837075591087, LR: 5e-05\n",
      " Epoch [174/300], Loss: 0.051253344491124155, LR: 5e-05\n",
      " Epoch [175/300], Loss: 0.049446435272693635, LR: 5e-05\n",
      " Epoch [176/300], Loss: 0.05123190172016621, LR: 5e-05\n",
      " Epoch [177/300], Loss: 0.05591992251574993, LR: 5e-05\n",
      " Epoch [178/300], Loss: 0.05550310462713241, LR: 5e-05\n",
      " Epoch [179/300], Loss: 0.053846026211977004, LR: 5e-05\n",
      " Epoch [180/300], Loss: 0.05413688942790031, LR: 5e-05\n",
      " Epoch [181/300], Loss: 0.05214649923145771, LR: 5e-05\n",
      " Epoch [182/300], Loss: 0.04993096254765987, LR: 5e-05\n",
      " Epoch [183/300], Loss: 0.0506051529198885, LR: 5e-05\n",
      " Epoch [184/300], Loss: 0.048510387539863586, LR: 5e-05\n",
      " Epoch [185/300], Loss: 0.04655781760811806, LR: 5e-05\n",
      " Epoch [186/300], Loss: 0.04680636487901211, LR: 5e-05\n",
      " Epoch [187/300], Loss: 0.047006918117403984, LR: 5e-05\n",
      " Epoch [188/300], Loss: 0.04668756537139416, LR: 5e-05\n",
      " Epoch [189/300], Loss: 0.04477365501224995, LR: 5e-05\n",
      " Epoch [190/300], Loss: 0.04610135070979595, LR: 5e-05\n",
      " Epoch [191/300], Loss: 0.04653494954109192, LR: 5e-05\n",
      " Epoch [192/300], Loss: 0.046558701619505885, LR: 5e-05\n",
      " Epoch [193/300], Loss: 0.04592148177325726, LR: 5e-05\n",
      " Epoch [194/300], Loss: 0.047749652341008184, LR: 5e-05\n",
      " Epoch [195/300], Loss: 0.04577401168644428, LR: 5e-05\n",
      " Epoch [196/300], Loss: 0.04664707817137241, LR: 5e-05\n",
      " Epoch [197/300], Loss: 0.046038641780614856, LR: 5e-05\n",
      " Epoch [198/300], Loss: 0.04584053009748459, LR: 5e-05\n",
      " Epoch [199/300], Loss: 0.046628975123167035, LR: 5e-05\n",
      " Epoch [200/300], Loss: 0.04663519859313965, LR: 3.5e-05\n",
      " Epoch [201/300], Loss: 0.04499181285500527, LR: 3.5e-05\n",
      " Epoch [202/300], Loss: 0.04561127237975597, LR: 3.5e-05\n",
      " Epoch [203/300], Loss: 0.04548319652676582, LR: 3.5e-05\n",
      " Epoch [204/300], Loss: 0.043592742457985875, LR: 3.5e-05\n",
      " Epoch [205/300], Loss: 0.042186159268021585, LR: 3.5e-05\n",
      " Epoch [206/300], Loss: 0.04188072085380554, LR: 3.5e-05\n",
      " Epoch [207/300], Loss: 0.04410620108246803, LR: 3.5e-05\n",
      " Epoch [208/300], Loss: 0.04338532947003841, LR: 3.5e-05\n",
      " Epoch [209/300], Loss: 0.04308809451758862, LR: 3.5e-05\n",
      " Epoch [210/300], Loss: 0.04203023090958595, LR: 3.5e-05\n",
      " Epoch [211/300], Loss: 0.041855699568986895, LR: 3.5e-05\n",
      " Epoch [212/300], Loss: 0.04457224048674106, LR: 3.5e-05\n",
      " Epoch [213/300], Loss: 0.041205520555377004, LR: 3.5e-05\n",
      " Epoch [214/300], Loss: 0.040693536028265954, LR: 3.5e-05\n",
      " Epoch [215/300], Loss: 0.04245583936572075, LR: 3.5e-05\n",
      " Epoch [216/300], Loss: 0.041275594383478165, LR: 3.5e-05\n",
      " Epoch [217/300], Loss: 0.04219614528119564, LR: 3.5e-05\n",
      " Epoch [218/300], Loss: 0.042287328839302064, LR: 3.5e-05\n",
      " Epoch [219/300], Loss: 0.04267587885260582, LR: 3.5e-05\n",
      " Epoch [220/300], Loss: 0.041907132416963574, LR: 3.5e-05\n",
      " Epoch [221/300], Loss: 0.041634615138173105, LR: 3.5e-05\n",
      " Epoch [222/300], Loss: 0.04700879007577896, LR: 3.5e-05\n",
      " Epoch [223/300], Loss: 0.044828425720334054, LR: 3.5e-05\n",
      " Epoch [224/300], Loss: 0.043052743375301364, LR: 3.5e-05\n",
      " Epoch [225/300], Loss: 0.040949529968202114, LR: 3.5e-05\n",
      " Epoch [226/300], Loss: 0.04155484028160572, LR: 3.5e-05\n",
      " Epoch [227/300], Loss: 0.04011271335184574, LR: 3.5e-05\n",
      " Epoch [228/300], Loss: 0.04004474058747291, LR: 3.5e-05\n",
      " Epoch [229/300], Loss: 0.040199561044573784, LR: 3.5e-05\n",
      " Epoch [230/300], Loss: 0.04006352163851261, LR: 3.5e-05\n",
      " Epoch [231/300], Loss: 0.040213124826550484, LR: 3.5e-05\n",
      " Epoch [232/300], Loss: 0.040107285231351854, LR: 3.5e-05\n",
      " Epoch [233/300], Loss: 0.03961801789700985, LR: 3.5e-05\n",
      " Epoch [234/300], Loss: 0.03979896884411573, LR: 3.5e-05\n",
      " Epoch [235/300], Loss: 0.03954349122941494, LR: 3.5e-05\n",
      " Epoch [236/300], Loss: 0.039386894926428793, LR: 3.5e-05\n",
      " Epoch [237/300], Loss: 0.03966150209307671, LR: 3.5e-05\n",
      " Epoch [238/300], Loss: 0.04062214680016041, LR: 3.5e-05\n",
      " Epoch [239/300], Loss: 0.03931375127285719, LR: 3.5e-05\n",
      " Epoch [240/300], Loss: 0.040055344440042974, LR: 3.5e-05\n",
      " Epoch [241/300], Loss: 0.03951433636248112, LR: 3.5e-05\n",
      " Epoch [242/300], Loss: 0.039791020937263964, LR: 3.5e-05\n",
      " Epoch [243/300], Loss: 0.03938547559082508, LR: 3.5e-05\n",
      " Epoch [244/300], Loss: 0.03823782615363598, LR: 3.5e-05\n",
      " Epoch [245/300], Loss: 0.03846369162201881, LR: 3.5e-05\n",
      " Epoch [246/300], Loss: 0.03857759647071361, LR: 3.5e-05\n",
      " Epoch [247/300], Loss: 0.03910671137273312, LR: 3.5e-05\n",
      " Epoch [248/300], Loss: 0.03928003739565611, LR: 3.5e-05\n",
      " Epoch [249/300], Loss: 0.04082102198153734, LR: 3.5e-05\n",
      " Epoch [250/300], Loss: 0.045476176962256434, LR: 3.5e-05\n",
      " Epoch [251/300], Loss: 0.04191501848399639, LR: 3.5e-05\n",
      " Epoch [252/300], Loss: 0.04056843817234039, LR: 3.5e-05\n",
      " Epoch [253/300], Loss: 0.0398752111941576, LR: 3.5e-05\n",
      " Epoch [254/300], Loss: 0.03883486483246088, LR: 3.5e-05\n",
      " Epoch [255/300], Loss: 0.03922945261001587, LR: 3.5e-05\n",
      " Epoch [256/300], Loss: 0.03965495526790619, LR: 3.5e-05\n",
      " Epoch [257/300], Loss: 0.038503783941268924, LR: 3.5e-05\n",
      " Epoch [258/300], Loss: 0.037260695919394494, LR: 3.5e-05\n",
      " Epoch [259/300], Loss: 0.03809791598469019, LR: 3.5e-05\n",
      " Epoch [260/300], Loss: 0.03671791702508927, LR: 3.5e-05\n",
      " Epoch [261/300], Loss: 0.03818014040589333, LR: 3.5e-05\n",
      " Epoch [262/300], Loss: 0.03704324811697006, LR: 3.5e-05\n",
      " Epoch [263/300], Loss: 0.037559980899095534, LR: 3.5e-05\n",
      " Epoch [264/300], Loss: 0.03832804728299379, LR: 3.5e-05\n",
      " Epoch [265/300], Loss: 0.04065342992544174, LR: 3.5e-05\n",
      " Epoch [266/300], Loss: 0.03875708393752575, LR: 3.5e-05\n",
      " Epoch [267/300], Loss: 0.04380649067461491, LR: 3.5e-05\n",
      " Epoch [268/300], Loss: 0.04170607030391693, LR: 3.5e-05\n",
      " Epoch [269/300], Loss: 0.03951440397650004, LR: 3.5e-05\n",
      " Epoch [270/300], Loss: 0.03919310085475445, LR: 3.5e-05\n",
      " Epoch [271/300], Loss: 0.03747150078415871, LR: 3.5e-05\n",
      " Epoch [272/300], Loss: 0.03733754400163889, LR: 3.5e-05\n",
      " Epoch [273/300], Loss: 0.03768988903611899, LR: 3.5e-05\n",
      " Epoch [274/300], Loss: 0.036956523545086384, LR: 3.5e-05\n",
      " Epoch [275/300], Loss: 0.03635540660470724, LR: 3.5e-05\n",
      " Epoch [276/300], Loss: 0.03768642842769623, LR: 3.5e-05\n",
      " Epoch [277/300], Loss: 0.03698109332472086, LR: 3.5e-05\n",
      " Epoch [278/300], Loss: 0.03748423047363758, LR: 3.5e-05\n",
      " Epoch [279/300], Loss: 0.03824079316109419, LR: 3.5e-05\n",
      " Epoch [280/300], Loss: 0.03850163612514734, LR: 3.5e-05\n",
      " Epoch [281/300], Loss: 0.036142699792981146, LR: 3.5e-05\n",
      " Epoch [282/300], Loss: 0.03607664462178946, LR: 3.5e-05\n",
      " Epoch [283/300], Loss: 0.03679050207138061, LR: 3.5e-05\n",
      " Epoch [284/300], Loss: 0.03751569651067257, LR: 3.5e-05\n",
      " Epoch [285/300], Loss: 0.0363029045984149, LR: 3.5e-05\n",
      " Epoch [286/300], Loss: 0.03766527958214283, LR: 3.5e-05\n",
      " Epoch [287/300], Loss: 0.036884716525673863, LR: 3.5e-05\n",
      " Epoch [288/300], Loss: 0.036637096852064135, LR: 3.5e-05\n",
      " Epoch [289/300], Loss: 0.03518197610974312, LR: 3.5e-05\n",
      " Epoch [290/300], Loss: 0.037115875631570816, LR: 3.5e-05\n",
      " Epoch [291/300], Loss: 0.03718658108264208, LR: 3.5e-05\n",
      " Epoch [292/300], Loss: 0.03710222654044628, LR: 3.5e-05\n",
      " Epoch [293/300], Loss: 0.03544710632413626, LR: 3.5e-05\n",
      " Epoch [294/300], Loss: 0.03649227414280176, LR: 3.5e-05\n",
      " Epoch [295/300], Loss: 0.035140684619545934, LR: 3.5e-05\n",
      " Epoch [296/300], Loss: 0.03448276352137327, LR: 3.5e-05\n",
      " Epoch [297/300], Loss: 0.035008428245782854, LR: 3.5e-05\n",
      " Epoch [298/300], Loss: 0.03570785652846098, LR: 3.5e-05\n",
      " Epoch [299/300], Loss: 0.0364782053977251, LR: 3.5e-05\n",
      " Epoch [300/300], Loss: 0.03573605902493, LR: 3.5e-05\n",
      "Model weights saved to /root/CPP_Customize_Transfer_Learning/ckpt/demo/model_all_epochs_300_seed_2024.pth\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs, labels = load_data(config[\"input_csv_path\"], config[\"label_csv_path\"])\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "set_seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "autoencoder = torch.load(autoencoder_checkpoint)\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "if freeze:\n",
    "    for param in autoencoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "combined_model = CombinedModel(autoencoder, P_spectra_dim=config['P_spectra_dim'], output_dim=output_dim_combined, mlp_output_dim=output_dim_combined, embedding_dim=onehot_dim).to(device)\n",
    "\n",
    "criterion = get_loss_function(loss_function_name)\n",
    "optimizer = optim.Adam(\n",
    "    [param for param in combined_model.parameters() if param.requires_grad],\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "\n",
    "train_inputs = inputs\n",
    "train_labels = labels\n",
    "\n",
    "train_dataset = CustomDataset(train_inputs, train_labels, file_paths, sampling_frequency, random_seed, is_train=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=set_seed(random_seed))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()\n",
    "    total_loss = 0\n",
    "    for inputs_tr, labels_tr in train_loader:\n",
    "        inputs_tr, labels_tr = inputs_tr.to(device), labels_tr.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = combined_model(inputs_tr)\n",
    "        loss = criterion(outputs, labels_tr)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f' Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}, LR: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "\n",
    "# Save the model weights after each fold\n",
    "model_save_path = os.path.join(ckpt_dir, f\"model_{config['mol']}_epochs_{num_epochs}_seed_{random_seed}.pth\")\n",
    "torch.save(combined_model, model_save_path)\n",
    "print(f\"Model weights saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "model_path = \"/root/CPP_Customize_Transfer_Learning/ckpt/demo/model_all_epochs_300_seed_2024.pth\"\n",
    "input_csv_path = \"/root/CPP_Customize_Transfer_Learning/data/demo/test_x.csv\"\n",
    "label_csv_path = \"/root/CPP_Customize_Transfer_Learning/data/demo/test_y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE: 0.14366924738510345, MSE: 0.04585729283807755, RMSE: 0.21414315968080222, R2: 0.8899666493575359, Pearson: 0.95434617802537 random_seed=2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 加载测试数据\n",
    "    inputs, labels = load_data(input_csv_path, label_csv_path)\n",
    "    test_dataset = CustomDataset(inputs, labels, file_paths, sampling_frequency, random_seed, is_train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    \n",
    "    test_predictions = np.empty(0)\n",
    "    test_labels = np.empty(0)\n",
    "    # 进行预测\n",
    "    for sample_idx, (inputs_te, labels_te) in enumerate(test_loader):\n",
    "        inputs_te, labels_te = inputs_te.to(device), labels_te.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs_te)\n",
    "            test_predictions = np.concatenate((test_predictions, output.cpu().numpy().flatten()))\n",
    "            test_labels = np.concatenate((test_labels, labels_te.cpu().numpy().flatten()))\n",
    "\n",
    "    mae, mse, rmse, r2, pearson_corr = calculate_metrics_all(test_labels, test_predictions)\n",
    "    print(f'Average MAE: {mae}, MSE: {mse}, RMSE: {rmse}, R2: {r2}, Pearson: {pearson_corr} random_seed={random_seed}')\n",
    "\n",
    "\n",
    "predict()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
